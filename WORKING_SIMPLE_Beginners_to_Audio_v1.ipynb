{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WORKING_SIMPLE_Beginners_to_Audio_v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "AkwTFM8pZYxu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# <center>Freesound General-Purpose Audio Tagging Challenge</center>\n",
        "\n",
        "![Logo](https://upload.wikimedia.org/wikipedia/commons/3/3c/Freesound_project_website_logo.png)\n",
        "\n",
        "Freesound is a collaborative database of Creative Commons Licensed sounds. The aim of this competition is to classify audio files that cover real-world sounds from musical instruments, humans, animals, machines, etc. Few of the labels are: `Trumpet`, `Squeak`, `Meow`, `Applause` and `Finger_sapping`.  One of the challenges is that not all labels are manually verified. A creative solution should be able to partially rely on these *weak* annotations.\n",
        "\n",
        "Let's take a tour of the data visualization and model building through this kernel. If you like this work, please show your support by upvotes. Happy Kaggling!\n",
        "\n",
        "### Contents\n",
        "1. [Exploratory Data Analysis](#eda)\n",
        "    * [Loading data](#loading_data)\n",
        "    * [Distribution of Categories](#distribution)\n",
        "    * [Reading Audio Files](#audio_files)\n",
        "    * [Audio Length](#audio_length)\n",
        "2. [Building a Model using Raw Wave](#1d_model_building)\n",
        "    * [Model Discription](#1d_discription)\n",
        "    * [Configuration](#configuration)\n",
        "    * [DataGenerator class](#data_generator)\n",
        "    * [Normalization](#1d_normalization)\n",
        "    * [Training 1D Conv](#1d_training)\n",
        "    * [Ensembling 1D Conv Predictions](#1d_ensembling)\n",
        "3. [Introduction to MFCC](#intro_mfcc)\n",
        "    * [Generating MFCC using Librosa](#librosa_mfcc)\n",
        "4. [Building a Model using MFCC](#2d_model_building)\n",
        "    * [Preparing Data](#2d_data)\n",
        "    * [Normalization](#2d_normalization)\n",
        "    * [Training 2D Conv on MFCC](#2d_training)\n",
        "    * [Ensembling 2D Conv Predictions](#2d_ensembling)\n",
        "5. [Ensembling 1D Conv and 2D Conv Predictions](#1d_2d_ensembling)\n",
        "6. [Results and Conclusion](#conclusion)\n",
        "\n",
        "\n",
        "<a id=\"eda\"></a>\n",
        "## <center>1. Exploratory Data Analysis</center>"
      ]
    },
    {
      "metadata": {
        "id": "swmrcoVLZYxy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Change this to True to replicate the result\n",
        "#COMPLETE_RUN = False\n",
        "COMPLETE_RUN = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NxfrpMgEli7c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Downloading data from a Shared Google Drive zip file\n",
        "\n",
        "        \n",
        "##  * The file audio_train8.zip contains original training FreeSound Challenge data <font color=red> downsampled to 8 KHz</font> using subsample.py\n",
        "\n",
        "## * File train.csv contains the labelling information provided by FreeSounf "
      ]
    },
    {
      "metadata": {
        "id": "Y1RjxB0kkwFc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! pip install googledrivedownloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JRTWtpoRl5hN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "\n",
        "# Download audio data from a shared GoogleDrive file\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='19c_Pc9dC_E96AGxN5ZdB-5UeOCDpYPvW',\n",
        "                                    dest_path='./audio_train8.zip',\n",
        "                                    unzip=False)\n",
        "\n",
        "# Download csv data from a shared GoogleDrive file\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1wEUNo9A_2W29YD8HWGNPoshs6Irk4VzQ',\n",
        "                                    dest_path='./train.csv',\n",
        "                                    unzip=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rDSAb_L1mX49",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I4XmI9cmsEfT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Extract audio (wav) files into audio_train8 subdirectory"
      ]
    },
    {
      "metadata": {
        "id": "IJ6rfXE5maF0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! unzip -q audio_train8.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uy5LvYewsTWI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ... check that there are 9473 audio files"
      ]
    },
    {
      "metadata": {
        "id": "g3xL8Oywmtfz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls ./audio_train8 | wc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8CwNxY8cZYx-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"loading_data\"></a>\n",
        "### Loading CSV data"
      ]
    },
    {
      "metadata": {
        "id": "Hr4Tqs3TseKC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "First some imports"
      ]
    },
    {
      "metadata": {
        "id": "CUZYqItYZYyC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1001)\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "%matplotlib inline\n",
        "matplotlib.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rFvbINy5ZYyI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"train.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aVodxRcgZYyO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UzJaUHBkZYyU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Number of training examples=\", train.shape[0], \"  Number of classes=\", len(train.label.unique()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TcscIuI_ZYya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(train.label.unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gl8zEoPpZYyg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"distribution\"></a>\n",
        "### Distribution of Categories"
      ]
    },
    {
      "metadata": {
        "id": "C0IydmOwZYyi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "category_group = train.groupby(['label', 'manually_verified']).count()\n",
        "plot = category_group.unstack().reindex(category_group.unstack().sum(axis=1).sort_values().index)\\\n",
        "          .plot(kind='bar', stacked=True, title=\"Number of Audio Samples per Category\", figsize=(16,10))\n",
        "plot.set_xlabel(\"Category\")\n",
        "plot.set_ylabel(\"Number of Samples\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cdY4g6vLZYys",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Minimum samples per category = ', min(train.label.value_counts()))\n",
        "print('Maximum samples per category = ', max(train.label.value_counts()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pGzyu8fBZYy0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We observe that:\n",
        "1. The number of audio samples per category is **non-unform**. The minimum number of audio samples in a category is `94` while the maximum is `300`\n",
        "2. Also, the proportion of `maually_verified` labels per category is non-uniform.\n",
        "<a id=\"audio_files\"></a>\n",
        "### Reading Audio Files\n",
        "\n",
        "The audios are [Pulse-code modulated](https://en.wikipedia.org/wiki/Audio_bit_depth) with a [bit depth](https://en.wikipedia.org/wiki/Audio_bit_depth) of 16 and a [sampling rate](https://en.wikipedia.org/wiki/Sampling_%28signal_processing%29) of 8 kHz (NOT 44.1 kHz)\n",
        "\n",
        "![16-bit PCM](https://upload.wikimedia.org/wikipedia/commons/thumb/b/bf/Pcm.svg/500px-Pcm.svg.png)\n",
        "\n",
        "* **Bit-depth = 16**: The amplitude of each sample in the audio is one of 2^16 (=65536) possible values. \n",
        "* **Samplig rate = 8000 (NOT 44.1 kHz)**: Each second in the audio consists of **8000** not 44100 samples. So, if the duration of the audio file is 3.2 seconds, the audio will consist of 8000\\*3.2= 24000 values (44100\\*3.2 = 141120 values).\n",
        "\n",
        "Let's listen to an audio file in our dataset and load it to a numpy array"
      ]
    },
    {
      "metadata": {
        "id": "pcEs518lZYy4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd  # To play sound in the notebook\n",
        "fname = './audio_train8/' + '00353774.wav'   # Cello\n",
        "ipd.Audio(fname)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "28XSKSbwZYzC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Using wave library\n",
        "import wave\n",
        "wav = wave.open(fname)\n",
        "print(\"Sampling (frame) rate = \", wav.getframerate())\n",
        "print(\"Total samples (frames) = \", wav.getnframes())\n",
        "print(\"Duration = \", wav.getnframes()/wav.getframerate())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EplKL8kKZYzI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Using scipy\n",
        "from scipy.io import wavfile\n",
        "rate, data = wavfile.read(fname)\n",
        "print(\"Sampling (frame) rate = \", rate)\n",
        "print(\"Total samples (frames) = \", data.shape)\n",
        "print(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "byu0YS7SZYzO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's plot the audio frames"
      ]
    },
    {
      "metadata": {
        "id": "M_bfQgPfZYzQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(data, '-', );"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kwxg_YceZYzY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's zoom in on first 2000 samples"
      ]
    },
    {
      "metadata": {
        "id": "vTXdvvIwZYzc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16, 4))\n",
        "plt.plot(data[:2000], '.'); plt.plot(data[:2000], '-');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j2mPLoXrZYzg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"audio_length\"></a>\n",
        "### Audio Length\n",
        "\n",
        "We shall now analyze the lengths of the audio files in our dataset"
      ]
    },
    {
      "metadata": {
        "id": "2R5HQOGjZYzg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train['nframes'] = train['fname'].apply(lambda f: wave.open('./audio_train8/' + f).getnframes())\n",
        "\n",
        "\n",
        "_, ax = plt.subplots(figsize=(16, 4))\n",
        "sns.violinplot(ax=ax, x=\"label\", y=\"nframes\", data=train)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Distribution of audio frames, per label', fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CYmqfVAPZYzm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We observe:\n",
        "1. The distribution of audio length across labels is non-uniform and has high variance.\n",
        "\n",
        "Let's now analyze the frame length distribution in Train and Test."
      ]
    },
    {
      "metadata": {
        "id": "tYNc36qVZYzo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "train.nframes.hist(bins=100)\n",
        "plt.suptitle('Frame Length Distribution in Train', ha='center', fontsize='large');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XuORbgIKt3i2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Methodology: Train/Test .... by now, to make it simple:\n",
        "    Prepare Train and Test data from FreeSound Training DataSet"
      ]
    },
    {
      "metadata": {
        "id": "BctY-ZJeZY1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is important to convert raw labels to integer indices"
      ]
    },
    {
      "metadata": {
        "id": "PCRzgLFiZY1Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LABELS = list(train.label.unique())\n",
        "label_idx = {label: i for i, label in enumerate(LABELS)}\n",
        "train.set_index(\"fname\", inplace=True)\n",
        "train[\"label_idx\"] = train.label.apply(lambda x: label_idx[x])\n",
        "if not COMPLETE_RUN:\n",
        "    train = train[:2000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uOBRiyPbgvCI",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4KnRot1kts4Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0mu2rtV2uOxO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### - first prepare balanced test data:\n",
        "\n",
        "      test_df Pandas DataFrame"
      ]
    },
    {
      "metadata": {
        "id": "z40PwAtSuF6F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Select a test sample from TRAIN dataset with same number of audio per class\n",
        "\n",
        "n_samples_per_class=20\n",
        "test_sample=train.groupby('label')['label_idx'].apply(lambda x: x.sample(n=n_samples_per_class))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UkLvoqkUufru",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test_sample is a Multilevel panadas Series with the file names in the second level (1)\n",
        "\n",
        "\n",
        "test_list= list(test_sample.index.get_level_values(1))\n",
        "test_df=train.loc[train.index.isin(test_list)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E04S9JU1unHF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### - now let's take the rest of data as the Training dataset:\n",
        "\n",
        "    train_df Pandas DataFrame"
      ]
    },
    {
      "metadata": {
        "id": "fEmahfoMuulP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_df=train.loc[~train.index.isin(test_list)]\n",
        "\n",
        "print('Number of audios for final train: ',len(train_df))\n",
        "print('Number of audios for final test : ',len(test_df))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X11ozH1AZY0e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"configuration\"></a>\n",
        "#### Configuration"
      ]
    },
    {
      "metadata": {
        "id": "LYLtdhNWZY0e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Configuration object stores those learning parameters that are shared between data generators, models, and training functions. Anything that is `global` as far as the training is concerned can become the part of Configuration object."
      ]
    },
    {
      "metadata": {
        "id": "N3UrReZJZY0i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Config(object):\n",
        "    def __init__(self,\n",
        "                 sampling_rate=8000, audio_duration=2, n_classes=41,\n",
        "                 use_mfcc=False, n_folds=10, learning_rate=0.0001, \n",
        "                 max_epochs=50, n_mfcc=20):\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.audio_duration = audio_duration\n",
        "        self.n_classes = n_classes\n",
        "        self.use_mfcc = use_mfcc\n",
        "        self.n_mfcc = n_mfcc\n",
        "        self.n_folds = n_folds\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_epochs = max_epochs\n",
        "\n",
        "        self.audio_length = self.sampling_rate * self.audio_duration\n",
        "        if self.use_mfcc:\n",
        "            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length/512)), 1)\n",
        "        else:\n",
        "            self.dim = (self.audio_length, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q5DxSHQmiGad",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## <font color=red>See Librosa</font>"
      ]
    },
    {
      "metadata": {
        "id": "LVlVFl4PtqqX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Librosa Introduction\n",
        "\n",
        "<font color=FA9900 size=4>LibROSA is a python package for music and audio analysis. It provides the building blocks necessary to create music information retrieval systems.</font>\n",
        "\n",
        "- Tutorial home: http://librosa.github.io/librosa/tutorial.html\n",
        "- Librosa home: http://librosa.github.io/\n",
        "- User forum: https://groups.google.com/forum/#!forum/librosa"
      ]
    },
    {
      "metadata": {
        "id": "iqtuzHXNiGw6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! pip install -q librosa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wgApAZiGiNmy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import librosa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qbYz5UEcy4n8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Though it is better using generators, lets try reading ALL the Data at ONCE using prepare data"
      ]
    },
    {
      "metadata": {
        "id": "B_jhcFkCZY0u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"data_generator\"></a>\n",
        "#### DataGenerator Class"
      ]
    },
    {
      "metadata": {
        "id": "Fwxsc3ZhZY0w",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The DataGenerator class inherits from **`keras.utils.Sequence`** . It is useful for preprocessing and feeding the data to a Keras model. \n",
        "* Once initialized with a batch_size, it computes the number of batches in an epoch. The **`__len__`** method tells Keras how many batches to draw in each epoch. \n",
        "* The **`__getitem__`** method takes an index (which is the batch number) and returns a batch of the data (both X and y) after calculating the offset. During test time, only `X` is returned.\n",
        "* If we want to perform some action after each epoch (like shuffle the data, or increase the proportion of augmented data), we can use the **`on_epoch_end`** method.\n",
        "\n",
        "Note:\n",
        "**`Sequence`** are a safer way to do multiprocessing. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators."
      ]
    },
    {
      "metadata": {
        "id": "h4lDD2DzZY0y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, config, data_dir, list_IDs, labels=None, \n",
        "                 batch_size=64, preprocessing_fn=lambda x: x):\n",
        "        self.config = config\n",
        "        self.data_dir = data_dir\n",
        "        self.list_IDs = list_IDs\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.preprocessing_fn = preprocessing_fn\n",
        "        self.on_epoch_end()\n",
        "        self.dim = self.config.dim\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "        return self.__data_generation(list_IDs_temp)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        cur_batch_size = len(list_IDs_temp)\n",
        "        X = np.empty((cur_batch_size, *self.dim))\n",
        "\n",
        "        input_length = self.config.audio_length\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            file_path = self.data_dir + ID\n",
        "            \n",
        "            # Read and Resample the audio\n",
        "            data, _ = librosa.core.load(file_path, sr=self.config.sampling_rate,\n",
        "                                        res_type='kaiser_fast')\n",
        "\n",
        "            # Random offset / Padding\n",
        "            if len(data) > input_length:\n",
        "                max_offset = len(data) - input_length\n",
        "                offset = np.random.randint(max_offset)\n",
        "                data = data[offset:(input_length+offset)]\n",
        "            else:\n",
        "                if input_length > len(data):\n",
        "                    max_offset = input_length - len(data)\n",
        "                    offset = np.random.randint(max_offset)\n",
        "                else:\n",
        "                    offset = 0\n",
        "                data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
        "                \n",
        "            # Normalization + Other Preprocessing\n",
        "            if self.config.use_mfcc:\n",
        "                data = librosa.feature.mfcc(data, sr=self.config.sampling_rate,\n",
        "                                                   n_mfcc=self.config.n_mfcc)\n",
        "                data = np.expand_dims(data, axis=-1)\n",
        "            else:\n",
        "                data = self.preprocessing_fn(data)[:, np.newaxis]\n",
        "            X[i,] = data\n",
        "\n",
        "        if self.labels is not None:\n",
        "            y = np.empty(cur_batch_size, dtype=int)\n",
        "            for i, ID in enumerate(list_IDs_temp):\n",
        "                y[i] = self.labels[ID]\n",
        "            return X, to_categorical(y, num_classes=self.config.n_classes)\n",
        "        else:\n",
        "            return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "V58EoAGNy2CA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_data(df, config, data_dir):\n",
        "    if config.use_mfcc :\n",
        "      X = np.empty(shape=(df.shape[0], config.dim[0], config.dim[1], 1))\n",
        "    else:\n",
        "      X = np.empty(shape=(df.shape[0], config.dim[0], config.dim[1]))\n",
        "      \n",
        "    input_length = config.audio_length\n",
        "    for i, fname in enumerate(df.index):\n",
        "        # print(fname)\n",
        "        file_path = data_dir + fname\n",
        "        data, _ = librosa.core.load(file_path, sr=config.sampling_rate, res_type=\"kaiser_fast\")\n",
        "\n",
        "        # Random offset / Padding\n",
        "        if len(data) > input_length:\n",
        "            max_offset = len(data) - input_length\n",
        "            offset = np.random.randint(max_offset)\n",
        "            data = data[offset:(input_length+offset)]\n",
        "        else:\n",
        "            if input_length > len(data):\n",
        "                max_offset = input_length - len(data)\n",
        "                offset = np.random.randint(max_offset)\n",
        "            else:\n",
        "                offset = 0\n",
        "            data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n",
        "\n",
        "        if config.use_mfcc :\n",
        "          data = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc)\n",
        "          data = np.expand_dims(data, axis=-1)\n",
        "        else:\n",
        "          data = data[:, np.newaxis]\n",
        "            \n",
        "        X[i,] = data\n",
        "    return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zgTuDhSm0ShH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_df.index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bmipALzuhzeb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "config = Config(sampling_rate=8000, audio_duration=2, n_folds=10, learning_rate=0.001)\n",
        "if not COMPLETE_RUN:\n",
        "    config = Config(sampling_rate=8000, audio_duration=1, n_folds=2, learning_rate=0.01, max_epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N5S9BM7PzY7o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test = prepare_data(test_df, config, './audio_train8/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RZAcRO0hldV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KYA9Kqgflntc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train = prepare_data(train_df, config, './audio_train8/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zsBjqY-5mGuC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L3DyFWRpm2Tl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Labels: we will use one-hot-encoding from to_categorical provided by Keras"
      ]
    },
    {
      "metadata": {
        "id": "k4s2YpS0o1Jf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XbVYlNbwm_JU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_labels = to_categorical(list(test_df.label_idx), num_classes=config.n_classes)\n",
        "train_labels = to_categorical(list(train_df.label_idx), num_classes=config.n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4d0slzvNpNYu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('Train labels shape',train_labels.shape)\n",
        "print('Test labels shape',test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aYgn0jbXZY0U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"1d_model_building\"></a>\n",
        "## <center>2. Building a Model using Raw Wave</center>\n",
        "We will build two models:\n",
        "1. The first model will take the raw audio (1D array) as input and the primary operation will be Conv1D\n",
        "2. The second model will take the MFCCs as input. (We will explain MFCC later)\n",
        "\n",
        "<a id=\"1d_discription\"></a>\n",
        "### Keras Model using raw wave\n",
        "\n",
        "Our model has the architecture as follows:\n",
        "![raw](https://raw.githubusercontent.com/zaffnet/images/master/images/raw_model.jpg)\n",
        "\n",
        "**Important:**\n",
        "Due to the time limit on Kaggle Kernels, it is not possible to perform 10-fold training of a large model. I have trained the model locally and uploaded its output files as a dataset. If you wish to train the bigger model, change `COMPLETE_RUN = True` at the beginning of the kernel."
      ]
    },
    {
      "metadata": {
        "id": "DUMErghoZY0W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Some essential imports"
      ]
    },
    {
      "metadata": {
        "id": "bj-rphuNZY0Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import losses, models, optimizers\n",
        "from keras.activations import relu, softmax\n",
        "from keras.callbacks import (EarlyStopping, LearningRateScheduler,\n",
        "                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n",
        "from keras.layers import (Convolution1D, Dense, Dropout, GlobalAveragePooling1D, \n",
        "                          GlobalMaxPool1D, Input, MaxPool1D, concatenate)\n",
        "from keras.utils import Sequence, to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ohs9155EZY08",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"1d_normalization\"></a>\n",
        "#### Normalization\n",
        "\n",
        "Normalization is a crucial preprocessing step. The simplest method is rescaling the range of features to scale the range in [0, 1]. "
      ]
    },
    {
      "metadata": {
        "id": "RCnSsdWbZY0-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def audio_norm(data):\n",
        "    max_data = np.max(data)\n",
        "    min_data = np.min(data)\n",
        "    data = (data-min_data)/(max_data-min_data+1e-6)\n",
        "    return data-0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zrX2hPGBwUxS"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Using Keras sequential....\n",
        "\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "ODfEe97yw6zb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import MaxPooling1D, Dropout, Dense, Flatten, GlobalMaxPool1D\n",
        "\n",
        "from keras.layers import Convolution1D as Conv1D\n",
        "\n",
        "nclass = config.n_classes\n",
        "input_length = config.audio_length\n",
        "\n",
        "model = Sequential()\n",
        "# input: audio signal (input_length,1)\n",
        "# apply 16 convolution filters of length 9 each and relu activation\n",
        "model.add(Conv1D(1.....\n",
        "\n",
        "model.add(GlobalMaxPool1D.....\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "...\n",
        "model.add(Dense(nclass, activation='softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3xBr7uYyGOC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-S2P3ZIzBOD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from keras.optimizers import SGD\n",
        "opt = optimizers.Adam(config.learning_rate)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j7_FCRaJzt-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ".... USE...\n",
        ", batch_size=100, epochs=50 , \\\n",
        "shuffle=True\n",
        "\n",
        "....validation_data=(X_test, test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H-M7T7SDvC43",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## <font color=red>Now let's use:\n",
        "## Keras Functional API\n",
        "\n",
        "See this [Keras Functional API guide](https://keras.io/getting-started/functional-api-guide/)\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "6H3Zb36nvg1z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def simple_1d_conv_model(config):\n",
        "    \n",
        "    nclass = config.n_classes\n",
        "    input_length = config.audio_length\n",
        "    \n",
        "    inp = Input(shape=(input_length,1))\n",
        "    \n",
        "    .....\n",
        "    \n",
        "    \n",
        "\n",
        "    ....\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NU1sYOnKwCZ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = simple_1d_conv_model(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q3aEJ5_Ev5on",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "haUPPVZwwN2F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, train_labels, batch_size=100, epochs=50 , \\\n",
        "                    shuffle=True, validation_data=(X_test, test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S4kGfV6xz20O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## <font color=red> Now try more complex (or a simple! dumy?) architecture</font>"
      ]
    },
    {
      "metadata": {
        "id": "U21q1xbwZY1C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* The dummy model is just for debugging purpose.\n",
        "* Our 1D Conv model is fairly deep and is trained using Adam Optimizer with a learning rate of 0.0001"
      ]
    },
    {
      "metadata": {
        "id": "j_FD_5_dZY1I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_1d_dummy_model(config):\n",
        "    \n",
        "    nclass = config.n_classes\n",
        "    input_length = config.audio_length\n",
        "    \n",
        "    inp = Input(shape=(input_length,1))\n",
        "    x = GlobalMaxPool1D()(inp)\n",
        "    out = Dense(nclass, activation=softmax)(x)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    opt = optimizers.Adam(config.learning_rate)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
        "    return model\n",
        "\n",
        "def get_1d_conv_model(config):\n",
        "    \n",
        "    nclass = config.n_classes\n",
        "    input_length = config.audio_length\n",
        "    \n",
        "    inp = Input(shape=(input_length,1))\n",
        "    x = Convolution1D(16, 9, activation=relu, padding=\"valid\")(inp)\n",
        "    x = Convolution1D(16, 9, activation=relu, padding=\"valid\")(x)\n",
        "    x = MaxPool1D(16)(x)\n",
        "    x = Dropout(rate=0.1)(x)\n",
        "    \n",
        "    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n",
        "    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n",
        "    x = MaxPool1D(4)(x)\n",
        "    x = Dropout(rate=0.1)(x)\n",
        "    \n",
        "    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n",
        "    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n",
        "    x = MaxPool1D(4)(x)\n",
        "    x = Dropout(rate=0.1)(x)\n",
        "    \n",
        "    x = Convolution1D(256, 3, activation=relu, padding=\"valid\")(x)\n",
        "    x = Convolution1D(256, 3, activation=relu, padding=\"valid\")(x)\n",
        "    x = GlobalMaxPool1D()(x)\n",
        "    x = Dropout(rate=0.2)(x)\n",
        "\n",
        "    x = Dense(64, activation=relu)(x)\n",
        "    x = Dense(1028, activation=relu)(x)\n",
        "    out = Dense(nclass, activation=softmax)(x)\n",
        "\n",
        "    model = models.Model(inputs=inp, outputs=out)\n",
        "    opt = optimizers.Adam(config.learning_rate)\n",
        "\n",
        "    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-5Bta4mqqQBy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Simple train /test"
      ]
    },
    {
      "metadata": {
        "id": "zcFxt8cOqS9Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if COMPLETE_RUN:\n",
        "        model = get_1d_conv_model(config)\n",
        "else:\n",
        "        model = get_1d_dummy_model(config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TUq34EbZqkUp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ti-BHa2uqpSK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, train_labels, batch_size=100, epochs=50 , \\\n",
        "                    shuffle=True, validation_data=(X_test, test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OK69JydyZY1u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<a id=\"intro_mfcc\"></a>\n",
        "## <center> 3. Introduction to MFCC\n",
        "\n",
        "As we have seen in the previous section, our Deep Learning models are powerful enough to classify sounds from the raw audio. We do not require any complex feature engineering. But before the Deep Learning era, people developed techniques to extract features from audio signals. It turns out that these techniques are still useful. One such technique is computing the MFCC (Mel Frquency Cepstral Coefficients) from the raw audio. Before we jump to MFCC, let's talk about extracting features from the sound.\n",
        "\n",
        "If we just want to classify some sound, we should build features that are **speaker independent**. Any feature that only gives information about the speaker (like the pitch of their voice) will not be helpful for classification. In other words, we should extract features that depend on the \"content\" of the audio rather than the nature of the speaker. Also, a good feature extraction technique should mimic the human speech perception. We don't hear loudness on a linear scale. If we want to double the perceived loudness of a sound, we have to put 8 times as much energy into it. Instead of a linear scale, our perception system uses a log scale. \n",
        "\n",
        "Taking these things into account, Davis and Mermelstein came up with MFCC in the 1980's. MFCC mimics the logarithmic perception of loudness and pitch of human auditory system and tries to eliminate speaker dependent characteristics by excluding the fundamental frequency and their harmonics. The underlying mathematics is quite complicated and we will skip that. For those interested, here is the [detailed explanation](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/).\n",
        "\n",
        "![http://recognize-speech.com/images/FeatureExtraction/MFCC/MFCC_Flowchart.png](http://recognize-speech.com/images/FeatureExtraction/MFCC/MFCC_Flowchart.png)\n",
        "\n",
        "<a id=\"librosa_mfcc\"></a>\n",
        "#### Generating MFCC using Librosa\n",
        "The library librosa has a function to calculate MFCC. Let's compute the MFCC of an audio file and visualize it."
      ]
    },
    {
      "metadata": {
        "id": "bsovaQmzZY1w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "SAMPLE_RATE = 44100\n",
        "fname = '../input/freesound-audio-tagging/audio_train/' + '00044347.wav'   # Hi-hat\n",
        "wav, _ = librosa.core.load(fname, sr=SAMPLE_RATE)\n",
        "wav = wav[:2*44100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j_tt3wV4ZY10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mfcc = librosa.feature.mfcc(wav, sr = SAMPLE_RATE, n_mfcc=40)\n",
        "mfcc.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tQzMZrTLZY12",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(mfcc, cmap='hot', interpolation='nearest');"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}